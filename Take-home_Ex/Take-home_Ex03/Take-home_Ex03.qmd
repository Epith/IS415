---
title: "Take-Home Exercise 3"
author: "Kwee Cheng"
date: "October 31, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  freeze: true
categories: [Take-Home, Code]
---

## Overview and Objectives

In this take-home my aim is to evaluate the necessary R packages necessary to perform:

-   Global Measures of Spatial Autocorrelation

-   Local Measures of Spatial Autocorrelation

This is to be done on the data which is the different types of crimes in Malaysia on the district level which we would layer with income inequality of Malaysia.

This also serves to prototype the Shiny application UI and choosing the right type of components

## Data

-   **Income Inequality Data:** Household income inequality by district (<https://data.gov.my/data-catalogue/hh_inequality_district>)

-   **Annual Principal Labour Force Statistics by District:** Annual principal labour force statistics including unemployment and participation rates (<https://data.gov.my/data-catalogue/lfs_district>)

-   **Crime Data:** Crime rates by district (<https://data.gov.my/data-catalogue/crime_district>)

-   **Malaysia - Subnational Administrative Boundaries: (**<https://data.humdata.org/dataset/cod-ab-mys?>)

## Packages

-   **sf** provides a standardised way to work with spatial vector data (points, lines, polygons)

-   **spdep** focuses on spatial econometrics and spatial statistics

-   **tmap** create thematic maps

-   **tidyverse** for easy data manipulation and some visualisation

-   **knitr** facilitates the integration of R code and documentation in reproducible research reports

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse, knitr)
```

## Importing the Data

Before the UI prototyping can be done let's see what type of data we are dealing with so that we can better plan for the UI components to be used

Let's import the crime dataset

```{r}
crime <- read_csv("data/aspatial/crime_district.csv")
```

Let's import the income inequality as well

```{r}
income <- read_csv("data/aspatial/hh_inequality.csv")
```

Also import the annual principal labour force statistics

```{r}
labour <- read_csv("data/aspatial/lfs_district.csv")
```

```{r}
crime
```

From here we can identify some of the variables that we can use, that the user can interact with ***district, category, type, date, crimes***

```{r}
income
```

Likewise for income we have ***district, date, gini***

```{r}
labour
```

For labour we have ***district, date, lf, lf_employed, lf_unemployed, lf_outside, p_rate, u_rate, ep_ration***

## UI Design

For a shiny application in this course we work with three main components **`headerPanel`**, **`sidebarPanel`**, and **`mainPanel`**.

-   **Header Panel** : This is the topmost part of the UI where we can put a description of the application or have a navbar where you can navigate different pages. Each page leads to other group members work/part in this project

-   **Sidebar Panel**: This panel would mainly consist of the input controls that the user can play around with to change the map output in the **Main Panel.**

-   **Main Panel** : This is the primary area of the application and it typically contains outputs. The main panel displays the output (like maps, plots, tables, etc.) based on the input given in the sidebar panel.

![](images/clipboard-3154485869.png)

### Header Panel

For this we would like to put **navbarPage()** which shiny provides. This is so as to keep our project organised and it would be easier to navigate through the different pages that we would have

![](images/clipboard-4240030914.png)

### Side Panel

For this part it would be the input controls and given the potential variables the the data type we have identified: ***district, category, type, date, crimes, gini.***

Some of the potential input controls that could be used are:

![](images/clipboard-3620315000.png){width="414"}

![](images/clipboard-2234524494.png)

![](images/clipboard-2052511263.png){width="430"}

![](images/clipboard-1621641194.png){width="401"}

![](images/clipboard-3092468463.png){width="274"}

![](images/clipboard-2079632102.png)

![](images/clipboard-2775147435.png)

Something that our side panel that could look like given the variables that we are given:

![](images/clipboard-1116187746.png)

![](images/clipboard-123672809.png)

Another possibility is to have the filters in a card look for the different sections of the plot e.g. global and local moran would be grouped together in their own card and EHSA would get their own card

### Main Panel

Given that I am working with LISA maps and that having a comparison between two maps would be helpful for the user to visualise.

![](images/clipboard-1619717407.png)

Another thing to consider would be is to have a tabset, for if I want to showcase more maps. So that it will be easier for the user to navigate.

![](images/clipboard-1650925226.png)

This would also be roughly how our shiny application would look like with the different layouts

## Data Wrangling

Looking at the crime csv file there are rows with "all" or "All" as the data. This seems to be a summary of the different crimes or summary for the different districts for the different years. So let's remove the them

```{r}
excluded_column <- "date"
crime <- crime[!apply(crime[, !names(crime) %in% excluded_column] == "all", 1, any), ]
crime <- crime[!apply(crime[, !names(crime) %in% excluded_column] == "All", 1, any), ]
```

Let's also add a column called year to the different csv files, to that it would be easier to split up the data into the different years

```{r}
crime <- crime %>%
              mutate(year = year(date))

income <- income %>%
              mutate(year = year(date))

labour <- labour %>%
              mutate(year = year(date))
```

Let's load Malaysia shape file and transform the crs into Malaysia

```{r}
msia_sf <- read_sf(dsn = "data/geospatial/mys_adm_unhcr_20210211_shp", 
                 layer = "mys_admbnda_adm2_unhcr_20210211") %>%
  st_as_sf(coords =c(
    "longitude", "latitude"),
           crs = 4326) %>%
  st_transform(crs = 3168)
```

```{r}
st_crs(msia_sf)
```

### Hole in boundary file

Next check if there are any holes with the boundary file

```{r}
u_msia <- st_union(msia_sf)
plot(u_msia)
```

### Missing row

Let's do a check if there are any missing values in the crime data

```{r}
na <- crime %>%
  summarise(na_district = sum(is.na(district)),
            na_category = sum(is.na(category)),
            na_type = sum(is.na(type)),
            na_date = sum(is.na(date)),
            na_crimes = sum(is.na(crimes))
            )
print(na)
```

Let's also do a check for the income inequality data

```{r}
na <- income %>%
  summarise(na_district = sum(is.na(district)),
            na_date = sum(is.na(date)),
            na_gini = sum(is.na(gini))
            )
print(na)
```

And also for the labour data

```{r}
na <- labour %>%
  summarise(na_district = sum(is.na(district)),
            na_date = sum(is.na(date)),
            na_lf = sum(is.na(lf)),
            na_lf_unemployed = sum(is.na(lf_unemployed)),
            na_u_rate = sum(is.na(u_rate)),
            )
print(na)
```

### Left Join

#### Mismatch Districts

Having check everything else, let's check whether is there any issues with ***msia_sf*** and ***crime***

```{r}
#| eval: false
combined_data <- bind_cols(crime = sort(unique(crime$district)), msia_sf = sort(unique(msia_sf$ADM2_EN)))

# Create a new column to compare the values
combined_data <- combined_data %>%
  mutate(same_values = crime == msia_sf) %>% filter(same_values == FALSE)

# View the result
combined_data
```

This would generate an error regarding difference in the number of data, in the ***crime*** there are 159 districts while in ***msia_sf*** there are 144 districts.

Let's run another code to see the difference

```{r}
crime_unique <- data.frame(district = sort(unique(crime$district)))
msia_unique <- data.frame(ADM2_EN = sort(unique(msia_sf$ADM2_EN)))

# Find rows in crime_unique that don't have a match in msia_unique
unmatched_crime <- anti_join(crime_unique, msia_unique, by = c("district" = "ADM2_EN"))

# Find rows in msia_unique that don't have a match in crime_unique
unmatched_msia <- anti_join(msia_unique, crime_unique, by = c("ADM2_EN" = "district"))

# Combine results to see all mismatches

unmatched_crime
unmatched_msia
```

From here we can actually see which data is missing in which file

Let's see all the unique districts in the sf file

```{r}
sort(unique(msia_sf$ADM2_EN))
```

From here there is no easy way to fix this but to google the districts mentioned in crime and try to map it as close as close to the district in the sf file

```{r}
crime <- crime %>%
  mutate(district = recode(district,
                           # Johor Bahru mappings
                           "Iskandar Puteri" = "Johor Bahru",
                           "Nusajaya" = "Johor Bahru",
                           "Johor Bahru Selatan" = "Johor Bahru",
                           "Johor Bahru Utara" = "Johor Bahru",
                           "Seri Alam" = "Johor Bahru",
                           
                           # Bandar Baharu correction
                           "Bandar Bharu" = "Bandar Baharu",
                           
                           # WP Kuala Lumpur mappings
                           "Brickfields" = "WP. Kuala Lumpur",
                           "Cheras" = "WP. Kuala Lumpur",
                           "Dang Wangi" = "WP. Kuala Lumpur",
                           "Sentul" = "WP. Kuala Lumpur",
                           "Wangsa Maju" = "WP. Kuala Lumpur",
                           
                           # Seremban correction
                           "Nilai" = "Seremban",
                           
                           # Seberang Perai corrections
                           "Seberang Perai Selatan" = "S.P.Selatan",
                           "Seberang Perai Tengah" = "S.P. Tengah",
                           "Seberang Perai Utara" = "S.P. Utara",
                           
                           # Cameron Highlands correction
                           "Cameron Highland" = "Cameron Highlands",
                           
                           # Lipis correction
                           "Kuala Lipis" = "Lipis",
                           
                           # Kinta mappings
                           "Batu Gajah" = "Kinta",
                           "Ipoh" = "Kinta",
                           
                           # Ulu Perak mappings
                           "Gerik" = "Ulu Perak",
                           "Pengkalan Hulu" = "Ulu Perak",
      
                           
                           # Manjung correction
                           "Manjung" = "Manjung (Dinding)",
                           
                           # Larut Dan Matang mappings
                           "Selama" = "Larut Dan Matang",
                           "Taiping" = "Larut Dan Matang",
                           
                           # Kuala Kangsar correction
                           "Sungai Siput" = "Kuala Kangsar",
                           
                           # Batang Padang mappings
                           "Tanjong Malim" = "Batang Padang",
                           "Tapah" = "Batang Padang",
                           
                           # Perlis mappings
                           "Arau" = "Perlis",
                           "Kangar" = "Perlis",
                           "Padang Besar" = "Perlis",
                           
                           # Kinabatangan correction
                           "Kota Kinabatangan" = "Kinabatangan",
                           
                           # Samarahan correction
                           "Kota Samarahan" = "Samarahan",
                           
                           # Mukah correction
                           "Matu Daro" = "Mukah",
                           
                           # Kuching correction
                           "Padawan" = "Kuching",
                           
                           # Gombak correction
                           "Ampang Jaya" = "Gombak",
                           
                           # Ulu Langat correction
                           "Kajang" = "Ulu Langat",
                           
                           # Ulu Selangor correction
                           "Hulu Selangor" = "Ulu Selangor",
                           
                           # Klang mappings
                           "Klang Selatan" = "Klang",
                           "Klang Utara" = "Klang",
                           
                           # Petaling mappings
                           "Petaling Jaya" = "Petaling",
                           "Serdang" = "Petaling",
                           "Sg. Buloh" = "Petaling",
                           "Shah Alam" = "Petaling",
                           "Subang Jaya" = "Petaling",
                           "Sungai Buloh" = "Petaling",
                           
                           # Default to keep original name if no match
                           .default = district))
```

let's check again to see if altered the data correctly

```{r}
crime_unique <- data.frame(district = sort(unique(crime$district)))

# Find rows in crime_unique that don't have a match in msia_unique
unmatched_crime <- anti_join(crime_unique, msia_unique, by = c("district" = "ADM2_EN"))

unmatched_crime
```

As we plan to overlay with the labour data, let's do checks for that as well

```{r}
labour_unique <- data.frame(district = sort(unique(labour$district)))
msia_unique <- data.frame(ADM2_EN = sort(unique(msia_sf$ADM2_EN)))

# Find rows in crime_unique that don't have a match in msia_unique
unmatched_labour <- anti_join(labour_unique, msia_unique, by = c("district" = "ADM2_EN"))

# Find rows in msia_unique that don't have a match in crime_unique
unmatched_msia <- anti_join(msia_unique, labour_unique, by = c("ADM2_EN" = "district"))

# Combine results to see all mismatches

unmatched_labour
unmatched_msia
```

Let's change the districts in *labour* like what we did for *crime*

```{r}
labour <- labour %>%
  mutate(district = recode(district,
                           "Kulai" = "Kulaijaya",
                           # Seberang Perai corrections
                           "Seberang Perai Selatan" = "S.P.Selatan",
                           "Seberang Perai Tengah" = "S.P. Tengah",
                           "Seberang Perai Utara" = "S.P. Utara",
                           
                           # Ulu Perak mappings
                           "Hulu Perak" = "Ulu Perak",
                           
                           # Manjung correction
                           "Manjung" = "Manjung (Dinding)",
                           
                           "Maradong" = "Meradong",
                           "Tangkak" = "Ledang",
                           
                           # Default to keep original name if no match
                           .default = district))
```

Let's check if there are still any issues with the district for *labour*

```{r}
labour_unique <- data.frame(district = sort(unique(labour$district)))
msia_unique <- data.frame(ADM2_EN = sort(unique(msia_sf$ADM2_EN)))

# Find rows in crime_unique that don't have a match in msia_unique
unmatched_labour <- anti_join(labour_unique, msia_unique, by = c("district" = "ADM2_EN"))

unmatched_labour
```

Let's combine our labour data with our crimes data

```{r}
crime_labour <- crime %>%
        filter(year >= 2019 & year <= 2022) %>%
        left_join(labour, by = c("district","year")) %>%
        select(1:4,6,7,10,12,15)
```

Let's check for any empty rows before *left_join*

```{r}
na <- crime_labour %>%
  summarise(na_district = sum(is.na(district)),
            na_category = sum(is.na(category)),
            na_type = sum(is.na(type)),
            na_crimes = sum(is.na(crimes)),
            na_year = sum(is.na(year)),
            na_lf = sum(is.na(lf)),
            na_lf_unemployed = sum(is.na(lf_unemployed)),
            na_u_rate = sum(is.na(u_rate)),
            )
print(na)
```

There are NA values so let's remove them

```{r}
crime_labour <- na.omit(crime_labour)
```

Do another check

```{r}
na <- crime_labour %>%
  summarise(na_district = sum(is.na(district)),
            na_category = sum(is.na(category)),
            na_type = sum(is.na(type)),
            na_crimes = sum(is.na(crimes)),
            na_year = sum(is.na(year)),
            na_lf = sum(is.na(lf)),
            na_lf_unemployed = sum(is.na(lf_unemployed)),
            na_u_rate = sum(is.na(u_rate)),
            )
print(na)
```

Finally with combine it with our *msia_sf*

```{r}
msia <- left_join(msia_sf,crime_labour, by = c("ADM2_EN" = "district")) %>%
        select(1,6,16:23)

msia
```

### NA Values

Looking at this we could see some additional rows have been added. Let's see if there are any NA values

```{r}
na <- msia %>%
  summarise(na_district = sum(is.na(ADM2_EN)),
            na_category = sum(is.na(category)),
            na_type = sum(is.na(type)),
            na_crimes = sum(is.na(crimes)),
            na_year = sum(is.na(year)),
            na_lf = sum(is.na(lf)),
            na_lf_unemployed = sum(is.na(lf_unemployed)),
            na_u_rate = sum(is.na(u_rate)),
            )
print(na)
```

Let's remove the NA rows

```{r}
msia <- na.omit(msia)
```

Do another check

```{r}
na <- msia %>%
  summarise(na_district = sum(is.na(ADM2_EN)),
            na_category = sum(is.na(category)),
            na_type = sum(is.na(type)),
            na_crimes = sum(is.na(crimes)),
            na_year = sum(is.na(year)),
            na_lf = sum(is.na(lf)),
            na_lf_unemployed = sum(is.na(lf_unemployed)),
            na_u_rate = sum(is.na(u_rate)),
            )
print(na)
```

Let's check for duplicates as well

```{r}
duplicates <- msia %>%
    group_by(ADM2_EN, year, category, type, crimes) %>%
    filter(n() > 1)
if(nrow(duplicates) > 0) {
    print("Duplicate combinations found!")
    print(duplicates)
}
```

## Global Measures of Spatial Autocorrelation

### Calculating Neighbours and Weights

I would be defining neighbour's based on Queens contiguity, and also let's assign spatial weights to each neighbouring polygon

```{r}
#| eval: false
msia_nb_q <- st_contiguity(msia, queen=TRUE)
```

As this takes time let's write it to a rds file

```{r}
#| eval: false
write_rds(msia_nb_q, "data/rds/msia_nb_q.rds")
```

```{r}
#| echo: false
msia_nb_q <- read_rds("data/rds/msia_nb_q.rds")
```

### Computing Row-Standardised Weight Matrix

Next, we need to assign spatial weights to each neighboring polygon.

[`st_weights()`](https://rdrr.io/cran/sfdep/man/st_weights.html) function from `sfdep` pacakge can be used to supplement a neighbour list with spatial weights

```{r}
msia_wm_rs <- st_weights(msia_nb_q, style="W")
```

We will mutate the newly created neighbour list object `msia_nb_q` and weight matrix `msia_wm_rs` into our existing `msia`. The result will be a new object, which we will call `wm_q`

```{r}
wm_q <- msia %>%
  mutate(nb = msia_nb_q,
         wt = msia_wm_rs,
         .before = 1) 
```

### Global Moran’s I Test

To assess spatial autocorrelation in our dataset, or how the presence of crimes in a district may form clusters.

```{r}
global_moran_test(wm_q$crimes,
                  wm_q$nb,
                  wm_q$wt,
                  zero.policy = TRUE,
                  na.action=na.omit)
```

From the test, the positive moran’s I statistic suggests that there is clustering, or a degree of spatial autocorrelation. This might be expected as when committing a crime, you might run to a neighbouring area. To commit another crime so as to be harder to track.

We can also see that the P-value is small. From a frequentist approach, we can see that this is unlikely to have occured by chance.

To strengthen our findings, we run a monte-carlo simulation.

```{r}
set.seed(2932)
```

### Global Moran’s I permutation test

```{r}
global_moran_perm(wm_q$crimes,
           wm_q$nb,
           wm_q$wt,
           zero.policy = TRUE,
           nsim = 99,
           na.action=na.omit)
```

From the outputs above, we can observe that the Moran’s I statistic (after 1000 permutations) for the **0.12406** with a p-value \< **2.2e-16** which is similar to our previous result with low p-value which suggest that it did not happen randomly.

## Local Moran I

Local Indicators of Spatial Association, or LISA, let us evaluate clusters between districts. Where higher values denote that the region is more heavily influenced by its surroundings.

### Calculating Local Moran I

Calculating local Moran’s I statistics and append the results to the original dataframe as new columns.

```{r}
wm_q <- wm_q %>%
          mutate(local_moran = local_moran(
            crimes, nb, wt, nsim = 99, zero.policy=TRUE),
                 .before = 1) %>%
          unnest(local_moran)
```

### Visualising Local Moran I

```{r}
lisa <- wm_q
map1 <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of No of crimes",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

### LISA

The local indicator of spatial association (LISA) for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation. LISA map is a categorical map showing type of outliers and clusters. There are two types of outliers namely: **High-Low** and **Low-High** outliers. Likewise, there are two type of clusters namely: **High-High** and **Low-Low** cluaters.

-   **High-Low Outliers:** Provinces with a high value of crimes, surrounded by neighbouring districts with low values of crimes.

-   **Low-High Outliers**: Provinces with a low value of crimes, surrounded by neighbouring districts with high values of crimes.

-   **High-High Clusters**: Provinces with a high value of crimes, surrounded by neighbouring districts with high values of crimes.

-   **Low-Low Clusters**: Provinces with a low value of crimes, surrounded by neighbouring districts with low values of crimes.

```{r}
lisa_sig <- lisa %>% filter(p_ii_sim < 0.05)
```

```{r}
tm_shape(lisa) +
  tm_polygons() + 
  tm_borders(alpha = 0.5) + 
  tm_shape(lisa_sig) + 
  tm_fill("mean", title = "LISA class") +
  tm_borders(alpha = 0.4) +
  tm_layout(main.title = "LISA map of crimes", main.title.size = 1)
```

## Emerging Hot Spot Analysis

### Performing Emerging Hot Spot Analysis

EHSA we can—and likely should—incorporate the time-lag of our spatial neighbors. Secondly, there are classifications proposed by ESRI which help us understand how each location is changing over time. Both of these are handled by the emerging_hotspot_analysis() function.

This emerging_hotspot_analysis() takes a spacetime object x, and the quoted name of the variable of interested in .var at minimum. We can specify the number of time lags using the argument k which is set to 1 by default.

For this let's create a st_data without the geometry and only doing it for *assault* for category and *causing_injury* for type

```{r}
category_to_select <- c(
  "assault"
)
type_to_select <- c(
  "causing_injury"
)
msia_df <- msia %>% filter(category %in% category_to_select, type %in% type_to_select )
msia_df <- msia_df %>%
  select(year, crimes, ADM2_EN) %>%
  st_drop_geometry()
```

```{r}
msia_sf_filtered <- msia_sf %>%
  semi_join(msia_df, by = "ADM2_EN")
```

Next is to create a spacetime object

```{r}
msia_spt <- spacetime(msia_df, msia_sf,
                 .loc_col = "ADM2_EN",
                 .time_col = "year")
```

Let's check if it is indeed a spacetime object

```{r}
is_spacetime_cube(msia_spt)
```

```{r}
msia_df <- msia_df %>%
    complete(year = unique(year),
             ADM2_EN = unique(ADM2_EN),
             fill = list(crimes = 0))  # Fill missing values with 0 or NA as needed

print(paste("Number of years:", length(unique(msia_df$year))))
print(paste("Number of locations:", length(unique(msia_df$ADM2_EN))))
print(paste("Total rows:", nrow(msia_df)))
```

For our space time we are suppose to get 128\*4 = 512 rows but we are getting 584 rows. So there could be cases of duplications

```{r}
duplicates <- msia_df %>%
    group_by(year, ADM2_EN) %>%
    filter(n() > 1)
if(nrow(duplicates) > 0) {
    print("Duplicate combinations found!")
    print(duplicates)
}
```

Let's remove them

```{r}
msia_df <- msia_df %>%
    group_by(year, ADM2_EN) %>%
    summarise(crimes = mean(crimes, na.rm = TRUE)) %>%
    ungroup()
check_duplicates <- msia_df %>%
    group_by(year, ADM2_EN) %>%
    filter(n() > 1)
print("Number of remaining duplicates:")
print(nrow(check_duplicates))
```

Let's create the spacetime object again

```{r}
msia_spt <- spacetime(msia_df, msia_sf,
                 .loc_col = "ADM2_EN",
                 .time_col = "year")
```

```{r}
is_spacetime_cube(msia_spt)
```

Another possible error could be that the *msia_sf* has more locations than what is in *msia_df*. Let's check it

```{r}
n_locations_df <- length(unique(msia_df$ADM2_EN))
n_locations_sf <- length(unique(msia_sf$ADM2_EN))
print(n_locations_df)
print(n_locations_sf)
```

In fact it is true, so lets change the *msia_sf* to only include those in *msia_df*

```{r}
msia_sf_filtered <- msia_sf %>%
  semi_join(msia_df, by = "ADM2_EN")

n_locations_df <- length(unique(msia_df$ADM2_EN))
n_locations_sf <- length(unique(msia_sf_filtered$ADM2_EN))
print(n_locations_df)
print(n_locations_sf)
```

Create the spacetime object again

```{r}
msia_spt <- spacetime(msia_df, msia_sf_filtered,
                 .loc_col = "ADM2_EN",
                 .time_col = "year")
```

```{r}
is_spacetime_cube(msia_spt)
```

We will perform EHSA analysis by using [`emerging_hotspot_analysis()`](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html) of sfdep package. It takes a spacetime object `msia_spt`, and the name of the variable of interest `crimes` for .var argument.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = msia_spt, 
  .var = "crimes", 
  k = 1, 
  nsim = 99
)
```

We can then join them together

```{r}
ehsa_sf <- left_join(msia_sf, ehsa, by = c("ADM2_EN" = "location"))
```

### Visualisation with Bar Graph

```{r}
ggplot(data = ehsa,
       aes(y = classification,fill = classification)) +
  geom_bar(show.legend = FALSE)
```

### Visualising Distribution of EHSA (85% Confidence)

```{r}
EHSA_sig <- ehsa_sf %>%
  filter(p_value < 0.15) 
```

```{r}
tm_shape(ehsa_sf) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
  tm_shape(EHSA_sig) +
  tm_fill(col = "classification", title = "Classification") + 
  tm_borders(alpha = 0.4) +
  tm_layout(main.title = "EHSA (>85%)", main.title.size = 1)
```

## Testing for Shiny App

All of the code above works well for getting the basics of **Global and Local Spatial Autocorrelation and Emerging Hot Spot Analysis** however when it comes to a Shiny application. The input is more dynamic as the user is able to choose his/her own inputs to alter the map, to make their own interpretation. Also the code takes awhile to run due to the size of the dataset to be used.

### Filtering for Global and Local Spatial Correlation

#### Global Spatial

As Shiny Application is interactive and user chooses the input, it is essentially applying a filter on the dataset.

For ***msia*** we have

-   **ADM2_EN:** we can limit this to all or have the user to be able to select a mixture of districts, however it would not really make sense, given how spatial correlation is about clustering

-   **category:** there is only assault or property. So the user can select either or, or both

-   **type:** to show based on what category is chosen. User is able to select all or a mixture

-   **year:** For this it would be best to limit the user to only select 1 year or a max of 2 years. Given how long it takes to run the code for 4 years already

-   **p_ii:** This can also be adjusted in the form of a slider to show the confidence level where 95% confidence would be \< 0.05 and 85% confidence is \< 0.15. To show strong evidence.

Something of what the filter would look like

```{r}
#in this case no filter is applied for ADM2_EN, it already contains all
ADM2_EN_to_select <- c("all")
category_to_select <- c("assault")
type_to_select <- c("causing_injury")
year_to_select <- c("2019")
```

however for this i would like to only have the year filtered to have the worst case scenario for running time

```{r}
msia_filtered <- msia %>% filter(year %in% year_to_select, )
```

Afterwards we can perform our normal global and local

```{r}
msia_filtered_nb_q <- st_contiguity(msia_filtered, queen=TRUE)
```

This significantly cuts down the timing, but its still way longer than what a user would be willing to wait for. This would run for about 45 secs.

Limiting the category selection to either or might help it.

```{r}
msia_filtered <- msia %>% filter(year %in% year_to_select, category %in% category_to_select )
```

```{r}
msia_filtered_nb_q <- st_contiguity(msia_filtered, queen=TRUE)
```

This cuts down the timing by quite a fair bit but will it affect the output

```{r}
msia_filtered_wm_rs <- st_weights(msia_filtered_nb_q, style="W")
```

```{r}
wm_q_filtered <- msia_filtered %>%
  mutate(nb = msia_filtered_nb_q,
         wt = msia_filtered_wm_rs,
         .before = 1) 
```

```{r}
global_moran_test(wm_q_filtered$crimes,
                  wm_q_filtered$nb,
                  wm_q_filtered$wt,
                  zero.policy = TRUE)
```

From the test, we can still see a positive moran’s I statistic suggests that there is clustering, or a degree of spatial autocorrelation.

We can also see that the P-value is small.

We can run monte-carlo simulation as well

```{r}
global_moran_perm(wm_q_filtered$crimes,
           wm_q_filtered$nb,
           wm_q_filtered$wt,
           zero.policy = TRUE,
           nsim = 999,
           na.action=na.omit)
```

The values are about the same

#### Local Spatial

We can do **Local Moran I** as well

```{r}
wm_q_filtered <- wm_q_filtered %>%
          mutate(local_moran = local_moran(
            crimes, nb, wt, nsim = 999, zero.policy=TRUE),
                 .before = 1) %>%
          unnest(local_moran)
```

So the above works with our filters applied. However the thing to consider is to whether to allow the user to be able select both category or a single one. As having a single category cuts down the processing time by a lot. But having more data also helps to portray the big picture better. But at the cost of time but not by a lot. It would still run it in under 1 minute.

### Testing different types of plots

#### Different map arrangement

```{r}
lisa <- wm_q_filtered
map1 <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of No of crimes",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

```{r}
tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
          labels = c("< 0.001", "< 0.01", "< 0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_text("u_rate", size = 0.4, col = "black") +  # Adding `p_value_2` as a text label
  tm_layout(main.title = "p-values of Two Variables",
            main.title.size = 0.8)
```

Having this plot is one way to display the *p_ii* with the *u_rate* to see if there are any relation to unemployment rate and the high clustering.

```{r}
lisa <- wm_q_filtered
map1 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
          labels = c("< 0.001", "< 0.01", "< 0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "p-value of local Moran's I of No of crimes",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_fill("u_rate") + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "unemployment rate",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 1)
```

Having 2 Maps each filled with *p_ii* and *u_rate* and arranging them top and bottom might be easier to read

```{r}
lisa <- wm_q_filtered
map1 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
          labels = c("< 0.001", "< 0.01", "< 0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "p-value of local Moran's I of No of crimes",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_polygons() +
  tm_text("u_rate", size = 0.4, col = "black") +
  tm_layout(main.title = "unemployment rate",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 1)
```

Having the text itself seems to be a better choice as its clear as to what is the actual value. But having this arrangement of top and bottom doesn't seem to be best. Ultimately it might be the first plot with some adjustment of the text that seems to be the best

#### LISA Map

```{r}
lisa <- wm_q_filtered
lisa_sig <- lisa %>% filter(p_ii_sim < 0.05)
tm_shape(lisa) +
  tm_polygons() + 
  tm_borders(alpha = 0.5) + 
  tm_shape(lisa_sig) + 
  tm_fill("mean", title = "LISA class") +
  tm_text("u_rate", size = 0.5, col = "black") +
  tm_borders(alpha = 0.4) +
  tm_layout(main.title = "LISA map of crimes", main.title.size = 1)
```

#### Testing with different figure size

```{r}
#| fig-height: 8
#| fig-width: 12
lisa <- wm_q_filtered
lisa_sig <- lisa %>% filter(p_ii_sim < 0.05)
tm_shape(lisa) +
  tm_polygons() + 
  tm_borders(alpha = 0.5) + 
  tm_shape(lisa_sig) + 
  tm_fill("mean", title = "LISA class") +
  tm_text("u_rate", size = 0.6, col = "black") +
  tm_borders(alpha = 0.4) +
  tm_layout(main.title = "LISA map of crimes", main.title.size = 1)
```

```{r}
#| fig-height: 14
#| fig-width: 15
lisa <- wm_q_filtered
lisa_sig <- lisa %>% filter(p_ii_sim < 0.05)
tm_shape(lisa) +
  tm_polygons() + 
  tm_borders(alpha = 0.5) + 
  tm_shape(lisa_sig) + 
  tm_fill("mean", title = "LISA class") +
  tm_text("u_rate", size = 0.8, col = "black") +
  tm_borders(alpha = 0.4) +
  tm_layout(main.title = "LISA map of crimes", main.title.size = 1)
```

#### Testing interactive maps

```{r}
tmap_mode("view")
tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
          labels = c("< 0.001", "< 0.01", "< 0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_text("u_rate", size = 0.6, col = "black") +  # Adding `p_value_2` as a text label
  tm_layout(main.title = "p-values of Two Variables",
            main.title.size = 0.8)
tmap_mode("plot")
```

### Filtering for Emerging Hot Spot Analysis

For this we would only be able to filter

-   **Category:** For this there would only be two options, selecting only either one

-   **Type:** User would only be able to select one based on the category they have chosen

Example of what the filter would look like

```{r}
category_to_select <- c(
  "property"
)
type_to_select <- c(
  "break_in"
)
```

We then repeat the steps like we did previously

```{r}
msia_df <- msia %>% filter(category %in% category_to_select, type %in% type_to_select )
msia_df <- msia_df %>%
  select(year, crimes, ADM2_EN) %>%
  st_drop_geometry()
```

```{r}
msia_sf_filtered <- msia_sf %>%
  semi_join(msia_df, by = "ADM2_EN")
```

```{r}
msia_df <- msia_df %>%
    group_by(year, ADM2_EN) %>%
    summarise(crimes = mean(crimes, na.rm = TRUE)) %>%
    ungroup()
check_duplicates <- msia_df %>%
    group_by(year, ADM2_EN) %>%
    filter(n() > 1)
```

```{r}
msia_spt <- spacetime(msia_df, msia_sf_filtered,
                 .loc_col = "ADM2_EN",
                 .time_col = "year")
```

```{r}
is_spacetime_cube(msia_spt)
```

```{r}
ehsa <- emerging_hotspot_analysis(
  x = msia_spt, 
  .var = "crimes", 
  k = 1, 
  nsim = 99
)
```

```{r}
ehsa_sf <- left_join(msia_sf, ehsa, by = c("ADM2_EN" = "location"))
```

```{r}
EHSA_sig <- ehsa_sf %>%
  filter(p_value < 0.15) 
```

```{r}
tm_shape(ehsa_sf) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
  tm_shape(EHSA_sig) +
  tm_fill(col = "classification", title = "Classification") + 
  tm_borders(alpha = 0.4) +
  tm_layout(main.title = "EHSA (>85%)", main.title.size = 1)
```

### Saving files to RDS

For now one of the things that is useful for us to save to rds would be the *msia* object as that requires a lot of pre processing

```{r}
#| eval: false
write_rds(msia, "data/rds/msia.rds")
```

## Proposed Shiny Application

![](images/clipboard-1058957239.png)

![](images/clipboard-223710064.png)

The parameters in this prototype view is based on what was tested in the **Testing for Shiny App** section. More may be added as deemed necessary or removed during development.
